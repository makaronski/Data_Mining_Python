{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Data\n",
    "\n",
    "In this session, we will use classifiers to detect email spam. The dataset is provided by the authors of the book [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/). The dataset has got 57 continuous features and a binary response. Since the elements are separated by spaces, we will use the regular expression for whitespace characters to import the data into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9  ...     48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...   0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...   0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  Spam  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278     1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028     1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259     1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191     1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data', \n",
    "                 engine='python', sep='\\\\s', header=None)\n",
    "feat_index = list(range(57))\n",
    "df.columns = feat_index+['Spam']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation: Shuffle Split\n",
    "Instead of using k-fold cross validation, we will use a simple shuffle split that randomly separates the data into 3 parts training data and 1 part test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[feat_index].values\n",
    "y = df.Spam.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote $P(y=k)$ as the (prior) probability of observation $y$ being a member of class $k$, $P(x=X)$ as the probability of observing $x$, and $P(X=k|y=k)$ as the (posterior) probability of observing $x$ given class $y$. Then using Bayes's rule, we can define the conditional probability of an observation belonging to class $k$ given $x$ as\n",
    "\n",
    "$$P(y=k|X=k)= \\frac{P(Y=y)P(X=x|y=k)}{P(X=x)}$$\n",
    "\n",
    "A new observation $x$ can be classified by choosing the class $k$, for which $P(y=k|X=k)$ is largest. Since $P(X=x)$ is independent of $k$, we can drop the denominator and select the class $k$ that maximizes\n",
    "\n",
    "$$P(y=k|X=x) = P(y=k) P(X=x|y=k)$$\n",
    "\n",
    "The Bayes classfier has the lowest possible error rate among all classfiers if we can specify the correct probabilities. Since this is rarely possible for real data, we must make certain assumptions about the data and develop an approximation of the Bayes classfier. One such approach is linear discriminant analysis (LDA).\n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "If we assume that the conditional probability of class membership follows a normal distribution with identical covariance matrix, the Bayes optimal classifier is to assign an observation to the class with the higher liklihood. A more detailed explanation can be found in Chapters 4.4.2 and 4.4.3 of [Introduction to Statsitical Learning](http://www-bcf.usc.edu/~gareth/ISL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 89.7 percent'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\"accuracy: %.1f percent\"%(100*accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis\n",
    "\n",
    "If we drop the assumption of having one identical covariance matrix, but rather allow each class to have its covariance matrix, we arrive at quadratic discriminant analysis (QDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 83.8 percent'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = QuadraticDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\"accuracy: %.1f percent\"%(100*accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case QDA is worse than LDA, which shows that using a more complex learner is not always better, as it typically requires more parameters to set. In this case, we must estimate parameters for two covariance matrices as opposed to one.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "We can also go the other direction by assuming that all features are independent. In this case, the Bayes classifier can be greatly simplified and we no further need to estimate a covariance matrix, but rather need to compute mean and standard deviation for each feature and class. Although this approach represents an extreme simplification, which is why it is called Naive Bayes, the method scales extremely well across thousands of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 82.7 percent'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\"accuracy: %.1f percent\"%(100*accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it has much fewer parameters, Naive Bayes is at par with QDA. This demonstrates that despite its simplicity, Naive Bayes provides a good the benchmark against which any other more complex classifier can be compared. If a more complex learner does not outperform a simple learner, the simple learner is to be preferred. In our case Naive Bayes is to be preferred over QDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classifier\n",
    "\n",
    "One of the most basic classifiers is the nearest neighbor classifier. Unlike the Bayesian classifiers or logistic regression, nearest neighbor is not model based, but used the data itself for prediction. Nearest neighbor is quite simple. All it takes is some measure of similarity, like the distance between the feature vectors, then new observations are classified by taking the class (the majority class) of the nearest neighbor (neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 81.8 percent'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\"accuracy: %.1f percent\"%(100*accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbor classifier is highly sensitive towards the similarity measure. If, for instance, the Euclidean distance is used, features that have the larger scale also have a larger effect on the distance. This means that features with larger scales implicitly receive a higher weight than data with a smaller scale.\n",
    "\n",
    "For example, assume one feature is income with observations ranging from \\$10,000 to \\$250,000, whereas another features is temperature with observations ranging from -10Â°C to 40Â°C. Any new observation is compared with all other observations by measuring the Euclidean distance between the feature vectors, i.e.,\n",
    "\n",
    "$$d(x_1,x_2) = \\sqrt{\\sum_{j=1}^p (x_{1j}-x_{2j})^2}$$\n",
    "\n",
    "It is quite obvious that $(250000-10000)^2$ contributes more to the distance than of $(-10-40)^2$, even though both mark the boundaries of the sample space.\n",
    "\n",
    "### Scaling\n",
    "A popular solution to this problem is scaling, whereby the most common type of scaling is normalization.\n",
    "\n",
    "Denote $x_{ij}$ as the $j$-th predictor of the $i$-th observation, with $\\mu_j$ as its mean and $\\sigma_j$ as the standard deviation. Then the standardized observation $z_{ij}$ is given by\n",
    "\n",
    "$$ z_{ij}= \\frac{x_{ij}-\\mu_j}{\\sigma_j}, \\ j=1,\\dots,p,\\ i=1,\\dots,n$$\n",
    "\n",
    "Although the boundaries of the data may still deviate the average distance in each dimension is now equal to one. An alternative to using the standard deviation is to use the inter-quartile range, which may be beneficial in the presence of outliers. Also see: http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "Let us scale our data using scikit's *StandardScaler*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling the accuracy of the nearest neighbor classfier gas increased from about 80 to 90 percent, which is at par with LDA and logistic regression. (Note that the accuracy figures deviate depending on which observations are used for training and which for testing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 91.9 percent'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train_sc, y_train)\n",
    "accuracy = clf.score(X_test_sc, y_test)\n",
    "\"accuracy: %.1f percent\"%(100*accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Let us assume that the conditional probability of an observation being a member of class '0' or class '1' for a given set of features $X$ can be defined by the logistic function\n",
    "\n",
    "$$P(y=1|X=x) = \\frac{1}{1+\\exp(- b x)}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$P(y=0|X=x) = 1-P(y=1|X=x) = 1-\\frac{1}{1+\\exp(- b x)}$$\n",
    "\n",
    "The optimal parameters $b$ are those that maximize the liklihood of making the given observations. \n",
    "\n",
    "### Maximum Liklihood Estimator\n",
    "\n",
    "To maximize the liklihood, we must first specify the liklihood function, which in our case is the joint probability for all observation. Assuming that observations are independent and identically distributed, the joint probability is simply the product of the conditional probabilities defined by the logistic function. More specifically, the liklihood function is given by\n",
    "\n",
    "$$\\max \\mathcal{L}(b) \\equiv \\prod_{i:y_i=1} P(y_i=1|X=x_i)\\prod_{i:y_i=1} (1-P(y_i=1|X=x_i))$$\n",
    "\n",
    "Instead of working with the liklihood function directly, it is more convenient to take the natural logarith, which gives us the so-called log-liklihood\n",
    "\n",
    "$$\\max \\ln \\mathcal{L}(b) \\equiv \\sum_{i:y_i=1} \\ln P(y_i=1|X=x_i)\\sum_{i:y_i=1} \\ln (1-P(y_i=1|X=x_i))$$\n",
    "\n",
    "As for the case of linear regression, we take the partial derivative of the log-liklihood function with respect to each element of parameter vector $b$ to derive the gradient,\n",
    "\n",
    "$$\\nabla \\log \\mathcal{L}(b) = \\sum_i \\left(y_i-\\frac{1}{1+\\exp(- b x_i)}\\right) x_i$$\n",
    "\n",
    "The gradient points in the direction of steepest ascent towards the $b$ that maximizes the log-liklihood. Since the logarithm is a monotone increasing function, the value that maximizes the log-liklihood also maximizes liklihood.\n",
    "\n",
    "Running logistic regression in scikit-learn is as simple as using linear regression, since the main difficulties of estimating the parameters are hidden inside the *LogisticRegression* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 93.3 percent'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\"accuracy: %.1f percent\"%(100*accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent \n",
    "\n",
    "Much like with linear regression, we can use the gradient of the maximum liklihood problem to search for the optimal parameters of the logistic function.\n",
    "\n",
    "Instead of computing the full gradient, we again update the gradient after having obtained the fraction of the slope that corresponds to a randomly drawn observation $x_i$. \n",
    "\n",
    "1. Choose initial guess $\\hat{b}_0$, stepsize $\\alpha$ \n",
    "2. <b>for</b> k = 0, 1, 2, ... <b>do</b>\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Draw randomly $i \\sim U(1,n)$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\nabla \\log P(y_i=1|x_i;\\hat{b}_k) = \\left(y_i-(1+\\exp(- \\hat{b}_k X_i))^{-1}\\right) X_i$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{b}_{k+1} = \\hat{b}_k + \\alpha \\nabla \\log P(y_i=1|x_i;\\hat{b}_k)$ \n",
    "\n",
    "In contrast to SGD for linear regression, we do not multiply the gradient by $-1$ since we are looking for the maximum, so that it would be technically more correct to call this method stochastic gradient *ascent*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Finally, let us compare the performance of the different classifiers for the spam data set by looking at the distribution of correct classifications for a number of shuffle splits of the data. For this, we create our own function to measure the accuracy of a set of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_dist(clfs, X, y, n=10):\n",
    "    accuracy = np.zeros((n,len(clfs)))\n",
    "    columns = [clf.__class__.__name__ for clf in clfs]\n",
    "    for i in range(n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=31*i)\n",
    "        for j in range(len(clfs)):\n",
    "            clf = clfs[j]\n",
    "            clf.fit(X_train,y_train)\n",
    "            accuracy[i][j] = clf.score(X_test,y_test)\n",
    "    return pd.DataFrame(accuracy, columns=columns, index=range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neMgZLwCgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fcea128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_df = accuracy_dist([GaussianNB(),LinearDiscriminantAnalysis(),LogisticRegression()], X, y)\n",
    "accuracy_df = pd.concat(\n",
    "    (accuracy_df, accuracy_dist([KNeighborsClassifier()], StandardScaler().fit_transform(X), y)), axis=1)\n",
    "accuracy_df.plot(kind='kde', ylim=[0,60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Names: Durdic Milena, Saric Dragana, Petrov Petar, Kneringer Paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "import random, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9  ...     48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...   0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...   0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  Spam  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278     1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028     1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259     1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191     1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data', \n",
    "                 engine='python', sep='\\\\s', header=None)\n",
    "feat_index = list(range(57))\n",
    "df.columns = feat_index+['Spam']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[feat_index].values\n",
    "y = df.Spam.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1\n",
    "Use the *SGDClassifier* provided by scikit-learn. Set *loss='log'*, *learning_rate='constant'*, *alpha=0.0*, *random_state=191*. Set *n_iter=10*, so that the SGDClassifier will sweep the sample 10 times. The stepsize is defined by the parameter *eta0*. Use 5-fold cross validation to search for a good eta0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "We provide 2 approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we create a window of three x_1,x_2,x_3 eta0 and shift the window in the direction with the best accuracy. If the windows is not moving, we stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy has improved. 0.00001000\n",
      "Accuracy has not imporved. Stopping search for eta0.\n",
      "Best eta0: 0.00001000\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 0.00001\n",
    "opt_learning_rate = 0\n",
    "\n",
    "new_accuracy = 0\n",
    "old_accuracy = 0\n",
    "\n",
    "learning_rate = []\n",
    "learning_rate.append(initial_learning_rate - 0.000005)\n",
    "learning_rate.append(initial_learning_rate)\n",
    "learning_rate.append(initial_learning_rate + 0.000005)\n",
    "\n",
    "while True:\n",
    "    accuracy = [0, 0, 0]\n",
    "    for index, item in enumerate(learning_rate):\n",
    "        clf = SGDClassifier(\n",
    "            loss='log',\n",
    "            learning_rate='constant',\n",
    "            alpha=0.0,\n",
    "            random_state=191,\n",
    "            max_iter=10,\n",
    "            eta0=learning_rate[index], # The initial learning rate for the â€˜constantâ€™ or â€˜invscalingâ€™ schedules.\n",
    "        )\n",
    "\n",
    "        kf = KFold(n_splits=5)\n",
    "\n",
    "        for train, test in kf.split(X_train_sc):\n",
    "            clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "            accuracy[index] += clf.score(X_test, y_test)\n",
    "\n",
    "        accuracy[index] = 100*(accuracy[index]/5)\n",
    "        #print(\"average accuracy test set: %.1f percent\"%(accuracy[index]))\n",
    "    \n",
    "    \n",
    "    new_accuracy = max(accuracy)\n",
    "    if new_accuracy > old_accuracy:\n",
    "        old_accuracy = new_accuracy\n",
    "        \n",
    "        lr_index = accuracy.index(max(accuracy))\n",
    "        \n",
    "        opt_learning_rate = learning_rate[lr_index]\n",
    "        \n",
    "        learning_rate[0] = learning_rate[lr_index] - 0.000005\n",
    "        learning_rate[1] = learning_rate[lr_index]\n",
    "        learning_rate[2] = learning_rate[lr_index] + 0.000005\n",
    "        \n",
    "        print('Accuarcy has improved. %.8f'%opt_learning_rate)\n",
    "    else:\n",
    "        print('Accuracy has not imporved. Stopping search for eta0.')\n",
    "        print('Best eta0: %.8f'%opt_learning_rate)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we define a function with 3 parameters. In the function, we create a list of eta0 [x_1=min_eta0, x_2=x_1+step_size, ..., max_eta0] and calcualte the accuracy for each one in the list. Finally, we print the optimal eta0 and return a tuple (optimal_eta0, accuracy) where accuracy is a list of accuracies for each eta0 and Kfold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_optimal_eta0(min_eta0=0.00001, max_eta0=0.0001, step_size = 0.00001):\n",
    "    optimal_eta0 = 0\n",
    "    accuracy = []\n",
    "    eta0_ls = []\n",
    "    \n",
    "    \n",
    "    eta0_ls.append(min_eta0)\n",
    "    while eta0_ls[len(eta0_ls)-1] <= max_eta0:\n",
    "        tmp_eta0 = eta0_ls[len(eta0_ls)-1] + step_size\n",
    "        eta0_ls.append(tmp_eta0)\n",
    "    \n",
    "    \n",
    "    for eta0 in eta0_ls:\n",
    "        n_accuracy = []\n",
    "        \n",
    "        clf = SGDClassifier(\n",
    "            loss='log',\n",
    "            learning_rate='constant',\n",
    "            alpha=0.0,\n",
    "            random_state=191,\n",
    "            max_iter=10,\n",
    "            eta0=eta0, # The initial learning rate for the â€˜constantâ€™ or â€˜invscalingâ€™ schedules.\n",
    "        )\n",
    "\n",
    "        kf = KFold(n_splits=5)\n",
    "        for train, test in kf.split(X_train_sc):\n",
    "            clf.fit(X_train_sc[train], y_train[train])\n",
    "\n",
    "            kfold_accuracy = clf.score(X_test_sc, y_test)\n",
    "            n_accuracy.append(kfold_accuracy)\n",
    "\n",
    "        accuracy.append(n_accuracy)\n",
    "    \n",
    "    \n",
    "    optimal_eta0 = eta0_ls[accuracy.index(max(accuracy))] \n",
    "    print('The optimal eta0 is ' + str(optimal_eta0))\n",
    "    \n",
    "    return (optimal_eta0, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal eta0 is 0.008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.008,\n",
       " [[0.91920069504778457,\n",
       "   0.92528236316246737,\n",
       "   0.92006950477845351,\n",
       "   0.91833188531711551,\n",
       "   0.91833188531711551],\n",
       "  [0.92354474370112949,\n",
       "   0.93136403127715028,\n",
       "   0.92962641181581229,\n",
       "   0.92528236316246737,\n",
       "   0.92615117289313642],\n",
       "  [0.92354474370112949,\n",
       "   0.93483927019982627,\n",
       "   0.93657688966116426,\n",
       "   0.92875760208514335,\n",
       "   0.92701998262380536],\n",
       "  [0.9218071242397915,\n",
       "   0.93570807993049521,\n",
       "   0.94005212858384013,\n",
       "   0.93223284100781934,\n",
       "   0.92875760208514335],\n",
       "  [0.92006950477845351,\n",
       "   0.93831450912250214,\n",
       "   0.93918331885317119,\n",
       "   0.93049522154648134,\n",
       "   0.93310165073848828],\n",
       "  [0.91920069504778457,\n",
       "   0.93918331885317119,\n",
       "   0.9374456993918332,\n",
       "   0.92962641181581229,\n",
       "   0.93397046046915722],\n",
       "  [0.92093831450912256,\n",
       "   0.93831450912250214,\n",
       "   0.9374456993918332,\n",
       "   0.92875760208514335,\n",
       "   0.93570807993049521],\n",
       "  [0.92354474370112949,\n",
       "   0.9374456993918332,\n",
       "   0.9374456993918332,\n",
       "   0.92962641181581229,\n",
       "   0.93570807993049521],\n",
       "  [0.92354474370112949,\n",
       "   0.9374456993918332,\n",
       "   0.93657688966116426,\n",
       "   0.92962641181581229,\n",
       "   0.9374456993918332],\n",
       "  [0.92354474370112949,\n",
       "   0.93657688966116426,\n",
       "   0.9374456993918332,\n",
       "   0.93049522154648134,\n",
       "   0.93831450912250214]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_optimal_eta0(min_eta0=0.001, max_eta0=0.01, step_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Implement a variant of logistic regression using stochastic gradient ascent with AdaGrad stepsizes. Make sure that the sample size is 10 times the training set size, so that your classifier sees as many data points as the SGDClassifier.\n",
    "\n",
    "1. Choose initial guess $\\hat{b}_0$, stepsize $\\alpha$ \n",
    "2. <b>for</b> k = 0, 1, 2, ... <b>do</b>\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Draw randomly $i \\sim U(1,n)$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\nabla \\log P(y_i=1|x_i;\\hat{b}_k) = \\left(y_i-(1+\\exp(- \\hat{b}_k X_i))^{-1}\\right) X_i$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{b}_{k+1} = \\hat{b}_k + \\alpha \\nabla \\log P(y_i=1|x_i;\\hat{b}_k)$ \n",
    "\n",
    "In contrast to SGD for linear regression, we do not multiply the gradient by $-1$ since we are looking for the maximum, so that it would be technically more correct to call this method stochastic gradient *ascent*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_ada_grad(X=0, y=0, iterations=10):\n",
    "    # Initial guess = 0\n",
    "    # Init parameter vector with 0s.\n",
    "    b = np.zeros(X.shape[1])\n",
    "    # Init sum of squared gradients \n",
    "    g = 0\n",
    "    \n",
    "    # Number of rows in df\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    iterations = iterations * n\n",
    "    \n",
    "    for k in range(iterations):\n",
    "        # Randomly select one row\n",
    "        i = random.randint(0, n-1)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        grad = (y[i] - (1 + np.exp(-b.dot(X[i])))**-1)*(X[i])\n",
    "        \n",
    "        # Sum of previous gradients\n",
    "        g = g + (grad)**2\n",
    "        \n",
    "        # Step size for each dimension\n",
    "        alpha = (1/np.sqrt(g))\n",
    "        \n",
    "        # Calculate new paramter vector\n",
    "        b = np.add(b, alpha * grad)\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Compare your own classifier against SGDClassifier, LogisticRegression, and the optimal KNeighborsClassifier using the function *accuracy_dist()*. Note that, for this you should implement your own in its own class which must implement the functions *fit(X,y)* and *score(X,y)*. The trick is to use a class variable for the parameters, e.g., *self.beta*, which can be accessed in both functions. If creating a class seems to difficult a function that does both, fitting and scoring, will also do the trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we wrtie a class *SGDLogisticRegression*. The choosen implementation is straightforward. However, implementing *self.b=np.zeros(56)* is naive. But we choose it for the sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDLogisticRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.b = np.zeros(56)\n",
    "        \n",
    "    \n",
    "    # Fits the model and sets parameter vector\n",
    "    def fit(self, X, y, iterations=10):\n",
    "        self.b = self.sgd_ada_grad(X, y, iterations)\n",
    "        \n",
    "    \n",
    "    # Calculates and returns accuracy for a given data set\n",
    "    def score(self, X, y):\n",
    "        y_pre = self.predict(X)\n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(len(y_pre)):\n",
    "            if y_pre[i] == y[i]:\n",
    "                correct += 1\n",
    "                \n",
    "        accuracy = correct/float(len(y))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    # Predicts and returns predictions for a given data set\n",
    "    def predict(self, X):\n",
    "        y_pre = np.zeros(X.shape[0])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            y_pre[i] = 1.0 / (1.0 + math.exp(self.b.dot(X[i])))\n",
    "            \n",
    "            if y_pre[i] <= 0.5:\n",
    "                y_pre[i] = 1\n",
    "            else:\n",
    "                y_pre[i] = 0\n",
    "        \n",
    "        y_pre = y_pre.astype(int)\n",
    "        \n",
    "        return y_pre\n",
    "    \n",
    "    \n",
    "    # Fits a model using SGD AdaGrad Algorithm\n",
    "    def sgd_ada_grad(self, X=0, y=0, iterations=10):\n",
    "        # Initial guess = 0\n",
    "        # Init parameter vector\n",
    "        b = np.zeros(X.shape[1])\n",
    "        # Init sum of squared gradients \n",
    "        g = 0\n",
    "    \n",
    "        # Number of rows in df\n",
    "        n = X.shape[0]\n",
    "    \n",
    "        iterations = iterations * n\n",
    "    \n",
    "        for k in range(iterations):\n",
    "            # Randomly select one row\n",
    "            i = random.randint(0, n-1)\n",
    "        \n",
    "            # Calculate gradient\n",
    "            grad = (y[i] - (1 + np.exp(-b.dot(X[i])))**-1)*(X[i])\n",
    "        \n",
    "            # Sum of previous gradients\n",
    "            g = g + (grad)**2\n",
    "        \n",
    "            # Step size for each dimension\n",
    "            alpha = (1/np.sqrt(g))\n",
    "        \n",
    "            # Calculate new paramter vector\n",
    "            b = np.add(b, alpha * grad)\n",
    "        \n",
    "        self.b = b\n",
    "        \n",
    "        return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we test our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls_log = SGDLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls_log.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_log.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157254561251086"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_log.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare our implementation *SGDLogisticRegression()* with *SGDClassifier(),LogisticRegression(),KNeighborsClassifier()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_dist(clfs, X, y, n=10):\n",
    "    accuracy = np.zeros((n,len(clfs)))\n",
    "    columns = [clf.__class__.__name__ for clf in clfs]\n",
    "    for i in range(n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=31*i)\n",
    "        for j in range(len(clfs)):\n",
    "            clf = clfs[j]\n",
    "            clf.fit(X_train,y_train)\n",
    "            accuracy[i][j] = clf.score(X_test,y_test)\n",
    "    return pd.DataFrame(accuracy, columns=columns, index=range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/stefan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/stefan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61f1a7fc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_df = accuracy_dist(\n",
    "    [SGDLogisticRegression(),SGDClassifier(),LogisticRegression()],\n",
    "    X,\n",
    "    y)\n",
    "\n",
    "accuracy_df = pd.concat(\n",
    "    (accuracy_df, accuracy_dist([KNeighborsClassifier()],StandardScaler().fit_transform(X), y)),\n",
    "    axis=1)\n",
    "\n",
    "accuracy_df.plot(kind='kde', ylim=[0,60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation has the worst accuracy. We suspect the reasen for this may be the unscaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Use 5-fold cross validation to search for the optimal *n_neighbors* of the *KNeighborsClassifier*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach is generate a list of neighors where $[1, 2, 3, ...., n]$ with *n=max_neigbors* and comput for each one the accuracy for KFold. We print the best solution a return a tuple (optimal_neigbors, accuracy) where accuracy is an array of arrays $[[x_1, x_2, .., x_5], [x_1, x_2, .., x_5], ... ]$ where $ \\forall x_i$ are accuracies for a KFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_neigbors(max_neighbors=10):\n",
    "    optimal_neigbors = 0\n",
    "    accuracy = []\n",
    "\n",
    "    for n_neighbor in range(1, max_neighbors):\n",
    "        n_accuracy = []\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbor)\n",
    "\n",
    "        kf = KFold(n_splits=5)\n",
    "        for train, test in kf.split(X_train_sc):\n",
    "            clf.fit(X_train_sc[train], y_train[train])\n",
    "\n",
    "            kfold_accuracy = clf.score(X_test_sc, y_test)\n",
    "            n_accuracy.append(kfold_accuracy)\n",
    "\n",
    "        accuracy.append(n_accuracy)\n",
    "    \n",
    "    \n",
    "    optimal_neigbors = accuracy.index(max(accuracy)) + 1 \n",
    "    print('The optimal number of n_neighbors is ' + str(optimal_neigbors))\n",
    "    \n",
    "    return (optimal_neigbors, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of n_neighbors is 7\n"
     ]
    }
   ],
   "source": [
    "opt = find_optimal_neigbors(max_neighbors=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
